{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418a3d08",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Benchmark:= AES-T700\n",
      "Model:= HTnet\n",
      "Output_directory:= ./results/\n",
      "device := CPU\n",
      "**************************************************\n",
      "**********Downloading Model and Dataset)**********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=19c7g-MPtixhJfC3fohjPtHlfq1LepvD9\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=19c7g-MPtixhJfC3fohjPtHlfq1LepvD9&confirm=t&uuid=2fcc2da2-1f5d-4ddc-b7a8-df6011a7a321\n",
      "To: /home/behnam/Projects/HW-Trojan-UAP-Generation/trained_models/regular_models/pytorch/HTnet.zip\n",
      "100%|███████████████████████████████████████| 57.1M/57.1M [00:00<00:00, 100MB/s]\n",
      "Extracting : 100%|██████████████████████████████| 13/13 [00:00<00:00, 30.79it/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1BDKHnF3xKOAZsKxqE82c7_UOrY1kzZuS\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1BDKHnF3xKOAZsKxqE82c7_UOrY1kzZuS&confirm=t&uuid=4d709ba3-0575-4a51-9556-9bdfacadb210\n",
      "To: /home/behnam/Projects/HW-Trojan-UAP-Generation/trained_models/at_models/pytorch/HTnet.zip\n",
      "100%|██████████████████████████████████████| 57.2M/57.2M [00:00<00:00, 89.1MB/s]\n",
      "Extracting : 100%|██████████████████████████████| 13/13 [00:00<00:00, 29.83it/s]\n",
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?export=download&id=1AR3WDI0c6iwlpyOspeGKlOudHcutdBPD\n",
      "From (redirected): https://drive.google.com/uc?export=download&id=1AR3WDI0c6iwlpyOspeGKlOudHcutdBPD&confirm=t&uuid=d47e478b-ea1e-4a44-b48c-84572d8a7d9d\n",
      "To: /home/behnam/Projects/HW-Trojan-UAP-Generation/dataset/AES-T700_power_Temp25C.zip\n",
      "100%|█████████████████████████████████████████| 164M/164M [00:01<00:00, 114MB/s]\n",
      "Extracting : 100%|██████████████████████| 60021/60021 [00:13<00:00, 4611.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "******************Data is loading******************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-27 17:10:39.318663: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 320000000 exceeds 10% of free system memory.\n",
      "2024-09-27 17:10:41.131472: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 160000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************Data is loaded******************\n",
      "**************************************************\n",
      "**************************************************\n",
      "*****************Model Evaluation*****************\n",
      "Accuracy for class:     0 is 100.0 %\n",
      "Accuracy for class:     1 is 100.0 %\n",
      "Model Accuracy:          100.0%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "from utils.utils import create_directory\n",
    "from utils.utils import get_splited_list_of_files_and_scaler_HT\n",
    "from utils.utils import Data_Generator, summrize\n",
    "import numpy as np\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import sys, os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\"\n",
    "os.environ['TF_FORCE_GPU_ALLOW_GROWTH']='true'\n",
    "plt.rcParams['font.family'] = ['serif']\n",
    "plt.rcParams['font.serif'] = ['Times New Roman']\n",
    "import logging\n",
    "logging.getLogger(\"matplotlib.font_manager\").setLevel(logging.ERROR)\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from random import shuffle\n",
    "import utils.utils_HT as ht\n",
    "from utils.utils import get_splited_list_of_files_and_scaler_HT\n",
    "from utils.utils import Data_Generator, summrize\n",
    "from keras_contrib.layers import InstanceNormalization\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "import os\n",
    "from scipy.signal import butter, lfilter, firwin, remez, kaiser_atten, kaiser_beta\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";\n",
    "import statistics\n",
    "import math\n",
    "from random import randrange\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "import pdb\n",
    "from tqdm import tqdm\n",
    "from scipy.signal import firwin\n",
    "from contextlib import contextmanager\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import argparse\n",
    "import zipfile\n",
    "import gdown\n",
    "\n",
    "def lowpass_firwin(data , ntaps, highcut, fs, window='hamming'):\n",
    "    nyq = 0.5 * fs\n",
    "    b = firwin(ntaps, highcut, nyq=nyq, pass_zero=True,\n",
    "                  window=window, scale=False)\n",
    "    \n",
    "    y = list(np.convolve(np.ravel(np.array(data)), np.ravel(b), mode='same'))\n",
    "    return y\n",
    "\n",
    "\n",
    "def Preprocessing(benchmark = 'AES-T700', model_name = 'HTnet'):\n",
    "    match model_name:\n",
    "        case 'HTnet':\n",
    "            url_regular = 'https://drive.google.com/uc?export=download&id=19c7g-MPtixhJfC3fohjPtHlfq1LepvD9'\n",
    "            url_at = 'https://drive.google.com/uc?export=download&id=1BDKHnF3xKOAZsKxqE82c7_UOrY1kzZuS'\n",
    "        case 'ResNet-18':\n",
    "            url_regular = 'https://drive.google.com/uc?export=download&id=1Jod-TXjtz_xQxSUzeJu4G9hQoNCv1jQ6'\n",
    "            url_at = 'https://drive.google.com/uc?export=download&id=1gRAwVs5G8XjTsnvHsUgNngRZ-qdrJP5X'\n",
    "        case 'VGG-11':\n",
    "            url_regular = 'https://drive.google.com/uc?export=download&id=1S7ePyec-ol7C_JF4VWIaIkryvV6etiTW'\n",
    "            url_at = 'https://drive.google.com/uc?export=download&id=1qbl7UYnfHH65Nie9sSLfCAVex1ZDTwrp'\n",
    "        case 'SVM':\n",
    "            url_regular = 'https://drive.google.com/uc?export=download&id=1syjXUX5_v4pVnp0VG4D_JVg9PF9WBrHS'\n",
    "            url_at = 'https://drive.google.com/uc?export=download&id=168gD8k0pMX5ffFdZ8WT2uQMpIf04rE-x'\n",
    "        case default:\n",
    "            sys.exit('Error: ' + model_name + ' is not supported.')\n",
    "\n",
    "\n",
    "    if not os.path.isdir('./trained_models/regular_models/pytorch/' + model_name):\n",
    "        gdown.download(url_regular, './trained_models/regular_models/pytorch/', quiet=False)\n",
    "        with zipfile.ZipFile('./trained_models/regular_models/pytorch/' + model_name + '.zip', 'r') as zip_ref:\n",
    "            for member in tqdm(zip_ref.infolist(), desc='Extracting '):\n",
    "                try:\n",
    "                    zip_ref.extract(member, './trained_models/regular_models/pytorch/')\n",
    "                except zipfile.error as e:\n",
    "                    pass\n",
    "                \n",
    "    if not os.path.isdir('./trained_models/at_models/pytorch/' + model_name):\n",
    "        gdown.download(url_at, './trained_models/at_models/pytorch/', quiet=False)\n",
    "        with zipfile.ZipFile('./trained_models/at_models/pytorch/' + model_name + '.zip', 'r') as zip_ref:\n",
    "            for member in tqdm(zip_ref.infolist(), desc='Extracting '):\n",
    "                try:\n",
    "                    zip_ref.extract(member, './trained_models/at_models/pytorch/')\n",
    "                except zipfile.error as e:\n",
    "                    pass\n",
    "                \n",
    "    match benchmark:\n",
    "        case 'AES-T400':\n",
    "            url = 'https://drive.google.com/uc?export=download&id=1sxfSfYc-T_XCJCHENxbpqlvgDXvL7Ma_'\n",
    "        case 'AES-T500':\n",
    "            url = 'https://drive.google.com/uc?export=download&id=153kahK2z7O16rVwtyt0pG3PXnmgf9gJD'\n",
    "        case 'AES-T600':\n",
    "            url = 'https://drive.google.com/uc?export=download&id=1d_aixtyDY1qC-8Ij3XlV7HYyikj4MKCp'\n",
    "        case 'AES-T700':\n",
    "            url = 'https://drive.google.com/uc?export=download&id=1AR3WDI0c6iwlpyOspeGKlOudHcutdBPD'\n",
    "        case 'AES-T800':\n",
    "            url = 'https://drive.google.com/uc?export=download&id=1ZlFeANl4zllhKfjGqcUsdatgq3cRuta_'\n",
    "        case 'AES-T1800':\n",
    "            url = 'https://drive.google.com/uc?export=download&id=1HVBMFxq-XagymyPfK2n-XdAWrdZYFHF8'\n",
    "        case default:\n",
    "            sys.exit('Error: ' + benchmark + ' is not supported.')\n",
    "                \n",
    "    if not os.path.isdir('./dataset/' + benchmark + '_power_Temp25C'):\n",
    "        gdown.download(url, './dataset/', quiet=False)\n",
    "        with zipfile.ZipFile('./dataset/' + benchmark + '_power_Temp25C.zip', 'r') as zip_ref:\n",
    "            for member in tqdm(zip_ref.infolist(), desc='Extracting '):\n",
    "                try:\n",
    "                    zip_ref.extract(member, './dataset/')\n",
    "                except zipfile.error as e:\n",
    "                    pass\n",
    "    \n",
    "    \n",
    "def KerasDataPrep(benchmark = 'AES-T700', number_of_samples = 40000, batch_size = 20):\n",
    "\n",
    "    name_bms = benchmark + '_power_Temp25C'\n",
    "    dir2bms_folder = './dataset/'\n",
    "\n",
    "    dirs_to_files_train, dirs_to_files_test, label_train, label_test, scaler, input_shape = \\\n",
    "                get_splited_list_of_files_and_scaler_HT(dir2bms_folder = dir2bms_folder, name_bms=[name_bms], \n",
    "                                                        use_enabled_trojan_folder = False,folder_numbers= [1,2], number_of_training_for_scaler=100, number_of_samples = number_of_samples)\n",
    "    \n",
    "    train_generator = Data_Generator(dirs_to_files_train, label_train, batch_size=batch_size,\n",
    "                                         dir2bms_folder=dir2bms_folder, scaler=scaler)\n",
    "    val_generator = Data_Generator(dirs_to_files_test, label_test, batch_size=batch_size,\n",
    "                                           dir2bms_folder=dir2bms_folder, scaler=scaler)\n",
    "    nb_classes = len(np.unique(np.concatenate((label_train, label_test), axis=0)))\n",
    "\n",
    "    data_train = (np.array([\n",
    "                    np.loadtxt(dir_to_file, delimiter='\\0')\n",
    "                      for dir_to_file in train_generator.dirs_to_files]))\n",
    "\n",
    "    data_test = (np.array([\n",
    "                    np.loadtxt(dir_to_file, delimiter='\\0')\n",
    "                      for dir_to_file in val_generator.dirs_to_files]))\n",
    "    \n",
    "\n",
    "    data_train = tf.cast(tf.reshape(tf.convert_to_tensor([data_train]), np.shape(data_train)), tf.float32)\n",
    "    label_train = tf.cast(tf.reshape(tf.convert_to_tensor([label_train]), np.shape(label_train)), tf.float32)\n",
    "    data_test = tf.cast(tf.reshape(tf.convert_to_tensor([data_test]), np.shape(data_test)), tf.float32)\n",
    "    label_test = tf.cast(tf.reshape(tf.convert_to_tensor([label_test]), np.shape(label_test)), tf.float32)\n",
    "    \n",
    "    return data_train, label_train, data_test, label_test, input_shape, nb_classes\n",
    "            \n",
    "def TorchLoadModel(model_name, benchmark):\n",
    "\n",
    "    torch_model = torch.load('trained_models/regular_models/pytorch/' + model_name + '/' + benchmark + '/' + benchmark + '.pt', map_location=torch.device('cpu'));\n",
    "    torch_model.to(device)\n",
    "    return torch_model\n",
    "    \n",
    "def TorchDataPrep(model_name, input_data, label):\n",
    "\n",
    "    if(model_name in ['HTnet', 'SVM']):\n",
    "        batch_size = 20\n",
    "        my_dataset = TensorDataset(torch.FloatTensor(input_data.numpy()), torch.FloatTensor(label.numpy())) \n",
    "        data_loader = DataLoader(my_dataset, batch_size=batch_size)\n",
    "    else:\n",
    "        batch_size = 1\n",
    "        my_dataset  = TensorDataset((torch.reshape(torch.FloatTensor(input_data.numpy()), [input_data.shape[0],1,50, 50]).repeat([1,3,1,1])), torch.FloatTensor(label.numpy())) \n",
    "        data_loader = DataLoader(my_dataset, batch_size=batch_size)\n",
    "    return data_loader\n",
    "\n",
    "def ModelEvaluation(model_name, torch_model, data_loader):\n",
    "    classes = (0,1)\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    correct_pred_model = 0\n",
    "    total_pred_model   = 0\n",
    "    pred = []\n",
    "    target = []\n",
    "\n",
    "    for _, (data_test_batched, label_test_batched) in enumerate(data_loader):\n",
    "\n",
    "        if(model_name == 'SVM'):\n",
    "            pred = torch.argmax(torch_model.predict_proba(data_test_batched.type(torch.DoubleTensor).to(device)), axis = 1)\n",
    "            target  = torch.argmax(label_test_batched, axis = 1) \n",
    "        else:\n",
    "            pred = torch.argmax(torch_model(data_test_batched.to(device)), axis = 1)\n",
    "            target  = torch.argmax(label_test_batched, axis = 1) \n",
    "\n",
    "        for label, prediction in zip(target, pred.cpu().detach().numpy()):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "                correct_pred_model += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            total_pred_model += 1\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5d} is {accuracy:.1f} %')\n",
    "\n",
    "    accuracy_model = 100 * float(correct_pred_model) / total_pred_model\n",
    "    print(f'Model Accuracy:          {accuracy_model:.1f}%')\n",
    "    \n",
    "        \n",
    "def SyncAdversarialPatchGen(model_name, torch_model , data_loader, nb_epoch=5, eps=2, resolution = 0.1, gn=False, filter_in_loop=False, fs=100, fh=20, at_mode = False):\n",
    "    _, (data_test_batched, _)  = next(enumerate(data_loader))\n",
    "    batch_delta = torch.zeros_like(data_test_batched).to(device)\n",
    "\n",
    "    delta = []\n",
    "    if(model_name in ['SVM']): \n",
    "        delta = batch_delta[0]\n",
    "    else:\n",
    "        delta = batch_delta[0][0]\n",
    "\n",
    "    losses = []\n",
    "    batch_delta.requires_grad_()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "    def clamped_loss(output, target):\n",
    "              loss = torch.mean(loss_fn(output, target))\n",
    "              return loss\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(nb_epoch), disable=at_mode):\n",
    "#         print('epoch %i/%i' % (epoch + 1, nb_epoch))\n",
    "        eps_step = resolution\n",
    "        for _, (data_test_batched, label_test_batched) in enumerate(data_loader):\n",
    "            y_target = torch.Tensor(np.repeat([[1.0, 0.0]], label_test_batched.shape[0], axis=0)).to(device)\n",
    "          \n",
    "            if batch_delta.grad is not None:\n",
    "                batch_delta.grad.data.zero_()\n",
    "                if(model_name in ['HTnet', 'SVM']): \n",
    "                    batch_delta.data = delta.unsqueeze(0).repeat([data_test_batched.shape[0], 1])\n",
    "                else:\n",
    "                    batch_delta.data = torch.reshape(delta.unsqueeze(0), [1,1, 50, 50]).repeat([1, 3, 1 , 1])\n",
    "              \n",
    "            trace = torch.stack([data_test_batched[i].to(device) + batch_delta[i] if torch.argmax(label_test_batched, axis = 1)[i] == 1\n",
    "              else data_test_batched[i].to(device) for i in range(data_test_batched.shape[0])])\n",
    "          \n",
    "            if gn:\n",
    "                trace.data = trace + torch.randn(trace.shape).to(device)\n",
    "          \n",
    "            if(model_name in ['HTnet', 'SVM']):  \n",
    "                 if filter_in_loop:\n",
    "                    trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace.cpu().detach().numpy())]\n",
    "                    trace.data = torch.Tensor(trace_tmp).to(device)\n",
    "            else:\n",
    "                 if filter_in_loop:\n",
    "                    trace_tmp = torch.reshape(trace[0][0], [1, 2500])\n",
    "                    trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace_tmp.cpu().detach().numpy())]\n",
    "                    trace.data= torch.reshape(torch.Tensor(trace_tmp).to(device), [1, 1, 50, 50]).repeat([data_test_batched.shape[0],3,1,1]).to(device)\n",
    "\n",
    "                      \n",
    "            outputs = []\n",
    "            if(model_name in ['SVM']):      \n",
    "                outputs = torch_model.predict_proba(trace.type(torch.DoubleTensor))\n",
    "            else:\n",
    "                outputs = torch_model(trace)\n",
    "\n",
    "            loss = -clamped_loss(outputs, y_target)\n",
    "            losses.append(torch.mean(loss.detach().cpu()))\n",
    "            loss.backward()\n",
    "\n",
    "            if(batch_delta.grad is not None):\n",
    "                grad_sign = []\n",
    "                if(model_name in ['HTnet', 'SVM']):\n",
    "                    grad_sign = batch_delta.grad.data.mean(dim = 0).sign()\n",
    "                else:\n",
    "                    grad_sign = batch_delta.grad.data.mean(dim = 0).mean(dim = 0).sign()\n",
    "                    grad_sign = torch.reshape(grad_sign, [1, 2500])\n",
    "                    delta = torch.reshape(delta, [1, 2500])\n",
    "                    \n",
    "                delta = delta + grad_sign * eps_step \n",
    "                if filter_in_loop:\n",
    "                    delta.data = torch.Tensor(lowpass_firwin(delta.cpu().detach().numpy(), 1024 , fh, fs, window='hamming')).to(device)\n",
    "                delta = torch.clamp(delta, 0, eps)\n",
    "                batch_delta.grad.data.zero_()\n",
    "    return delta, losses\n",
    "\n",
    "\n",
    "def SyncModelEvaluation(model_name, torch_model, data_loader, delta, gn = False, filter_in_loop= False, fs=100, fh=20):\n",
    "    classes = (0,1)\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    correct_pred_model = 0\n",
    "    total_pred_model   = 0\n",
    "    pred = []\n",
    "    target = []\n",
    "\n",
    "    for _, (data_test_batched, label_test_batched) in enumerate(data_loader):\n",
    "        \n",
    "        if(model_name in ['HTnet', 'SVM']):\n",
    "            if np.isnan(delta[0].cpu().detach().numpy()):\n",
    "                delta = torch.zeros_like(delta).to(device)\n",
    "        else:\n",
    "            if np.isnan(delta[0].cpu().detach().numpy()):\n",
    "                delta = torch.zeros_like(delta).to(device)\n",
    "        \n",
    "        noise = []\n",
    "        if(model_name in ['HTnet', 'SVM']): \n",
    "            noise = torch.Tensor.repeat(torch.reshape(delta, [1, data_test_batched.shape[1]]), [data_test_batched.shape[0],1]).to(device)\n",
    "        else:\n",
    "            noise = torch.reshape(delta, [data_test_batched.shape[0], 1, 50, 50]).repeat([data_test_batched.shape[0],3,1,1]).to(device)\n",
    "\n",
    "\n",
    "        trace = torch.stack([data_test_batched[i].to(device) + noise[i] if torch.argmax(label_test_batched, axis = 1)[i] == 1\n",
    "              else data_test_batched[i].to(device) for i in range(data_test_batched.shape[0])])\n",
    "\n",
    "        if gn:\n",
    "            trace.data = trace + torch.randn(trace.shape).to(device)\n",
    "\n",
    "        if(model_name in ['HTnet', 'SVM']):  \n",
    "            if filter_in_loop:\n",
    "                trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace.cpu().detach().numpy())]\n",
    "                trace.data = torch.Tensor(trace_tmp).to(device)\n",
    "        else:\n",
    "            if filter_in_loop:\n",
    "                trace_tmp = torch.reshape(trace[0][0], [1, 2500])\n",
    "                trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace_tmp.cpu().detach().numpy())]\n",
    "                trace.data= torch.reshape(torch.Tensor(trace_tmp).to(device), [1, 1, 50, 50]).repeat([data_test_batched.shape[0],3,1,1]).to(device)\n",
    "\n",
    "\n",
    "        if(model_name == 'SVM'):\n",
    "            pred = torch.argmax(torch_model.predict_proba(trace.type(torch.DoubleTensor).to(device)), axis = 1)\n",
    "            target  = torch.argmax(label_test_batched, axis = 1) \n",
    "        else:\n",
    "            pred = torch.argmax(torch_model(trace.to(device)), axis = 1)\n",
    "            target  = torch.argmax(label_test_batched, axis = 1) \n",
    "\n",
    "        class_accuracy = []\n",
    "\n",
    "        for label, prediction in zip(target, pred.cpu().detach().numpy()):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "                correct_pred_model += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            total_pred_model += 1\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5d} is {accuracy:.1f} %')\n",
    "        class_accuracy.append(accuracy)\n",
    "\n",
    "    accuracy_model = 100 * float(correct_pred_model) / total_pred_model\n",
    "    print(f'Model Accuracy:          {accuracy_model:.1f}%')\n",
    "    \n",
    "    return class_accuracy\n",
    "    \n",
    "\n",
    "def UnsyncAdversarialPatchGen(model_name, torch_model , data_loader, nb_epoch=5, eps=6, resolution=0.1, gn=False, filter_in_loop=False, fs=100, fh=20):\n",
    "\n",
    "    _, (data_test_batched, _)  = next(enumerate(data_loader))\n",
    "\n",
    "    batch_delta = torch.zeros_like(data_test_batched).to(device)\n",
    "    delta = []\n",
    "    if(model_name in ['SVM']): \n",
    "        delta = batch_delta[0]\n",
    "    else:\n",
    "        delta = batch_delta[0][0]\n",
    "\n",
    "    losses = []\n",
    "    batch_delta.requires_grad_()\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction = 'none')\n",
    "\n",
    "    def clamped_loss(output, target):\n",
    "        loss = torch.mean(loss_fn(output, target))\n",
    "        return loss\n",
    "\n",
    "    \n",
    "    for epoch in tqdm(range(nb_epoch)):\n",
    "        eps_step = resolution\n",
    "\n",
    "        for _, (data_test_batched, label_test_batched) in enumerate(data_loader):\n",
    "            y_target = torch.Tensor(np.repeat([[1.0, 0.0]], label_test_batched.shape[0], axis=0)).to(device)\n",
    "             \n",
    "            shift = [randrange(10) for i in range(data_test_batched.shape[0])] \n",
    "\n",
    "            if batch_delta.grad is not None:\n",
    "                batch_delta.grad.data.zero_()\n",
    "                if(model_name in ['HTnet', 'SVM']): \n",
    "                    batch_delta.data = delta.unsqueeze(0).repeat([data_test_batched.shape[0], 1])\n",
    "                else:\n",
    "                    batch_delta.data = torch.reshape(delta.unsqueeze(0), [1,1, 50, 50]).repeat([1, 3, 1 , 1])\n",
    "\n",
    "            if(model_name in ['HTnet', 'SVM']): \n",
    "                a_p = torch.stack([torch.concat([batch_delta[i][shift[i]:]\n",
    "                                    ,batch_delta[i][:shift[i]]]) for i in range(data_test_batched.shape[0])])   \n",
    "            else:\n",
    "                batch_delta_tmp = torch.reshape(batch_delta, [1,3,1,2500])\n",
    "                batch_delta_tmp = batch_delta_tmp[0][0]\n",
    "                a_p = torch.stack([torch.concat([batch_delta_tmp[i][shift[i]:]\n",
    "                                   ,batch_delta_tmp[i][:shift[i]]]) for i in range(data_test_batched.shape[0])])      \n",
    "                a_p = torch.reshape(a_p, [1,1,50,50]).repeat([data_test_batched.shape[0], 3, 1 , 1]).to(device) \n",
    "\n",
    "            trace = torch.stack([data_test_batched[i].to(device) + a_p[i] if torch.argmax(label_test_batched, axis = 1)[i] == 1\n",
    "                else data_test_batched[i].to(device) for i in range(data_test_batched.shape[0])])\n",
    "\n",
    "            if gn:\n",
    "                trace.data = trace + torch.randn(trace.shape).to(device)\n",
    "\n",
    "            if(model_name in ['HTnet', 'SVM']):  \n",
    "                 if filter_in_loop:\n",
    "                    trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace.cpu().detach().numpy())]\n",
    "                    trace.data = torch.Tensor(trace_tmp).to(device)\n",
    "            else:\n",
    "                 if filter_in_loop:\n",
    "                    trace_tmp = torch.reshape(trace[0][0], [1, 2500])\n",
    "                    trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace_tmp.cpu().detach().numpy())]\n",
    "                    trace.data= torch.reshape(torch.Tensor(trace_tmp).to(device), [batch_size, 1, 50, 50]).repeat([data_test_batched.shape[0],3,1,1]).to(device)\n",
    "\n",
    "\n",
    "            outputs = []\n",
    "            if(model_name in ['SVM']):      \n",
    "                outputs = torch_model.predict_proba(trace.type(torch.DoubleTensor))\n",
    "            else:\n",
    "                outputs = torch_model(trace)\n",
    "            loss = -clamped_loss(outputs, y_target)\n",
    "            losses.append(torch.mean(loss.detach().cpu()))\n",
    "            loss.backward()\n",
    "\n",
    "            if(batch_delta.grad is not None):\n",
    "                grad_sign = []\n",
    "                if(model_name in ['HTnet', 'SVM']):\n",
    "                    grad_sign = batch_delta.grad.data.mean(dim = 0).sign()\n",
    "                else:\n",
    "                    grad_sign = batch_delta.grad.data.mean(dim = 0).mean(dim = 0).sign()\n",
    "                    grad_sign = torch.reshape(grad_sign, [1, 2500])\n",
    "                    delta = torch.reshape(delta, [1, 2500])\n",
    "                    \n",
    "                delta = delta + grad_sign * eps_step \n",
    "                if filter_in_loop:\n",
    "                    delta.data = torch.Tensor(lowpass_firwin(delta.cpu().detach().numpy(), 1024 , fh, fs, window='hamming')).to(device)\n",
    "\n",
    "                delta = torch.round(torch.clamp(delta, 0, eps), decimals=1)\n",
    "                batch_delta.grad.data.zero_()\n",
    " \n",
    "    return delta, losses\n",
    "\n",
    "\n",
    "\n",
    "def UnsyncModelEvaluation(model_name, torch_model, data_loader, delta, gn = False, filter_in_loop= False, fs=100, fh=20):\n",
    "    classes = (0,1)\n",
    "    # prepare to count predictions for each class\n",
    "    correct_pred = {classname: 0 for classname in classes}\n",
    "    total_pred = {classname: 0 for classname in classes}\n",
    "    correct_pred_model = 0\n",
    "    total_pred_model   = 0\n",
    "    pred = []\n",
    "    target = []\n",
    "    noise = []\n",
    "    a_p = []\n",
    "\n",
    "    for _, (data_test_batched, label_test_batched) in enumerate(data_loader):\n",
    "        \n",
    "        if(model_name in ['HTnet', 'SVM']):\n",
    "            if np.isnan(delta[0].cpu().detach().numpy()):\n",
    "                delta = torch.zeros_like(delta).to(device)\n",
    "        else:\n",
    "            if np.isnan(delta[0].cpu().detach().numpy()):\n",
    "                delta = torch.zeros_like(delta).to(device)\n",
    "\n",
    "        \n",
    "        shift = [randrange(10) for i in range(data_test_batched.shape[0])] \n",
    "\n",
    "        if(model_name in ['HTnet', 'SVM']):\n",
    "            noise = torch.Tensor.repeat(torch.reshape(delta, [1, data_test_batched.shape[1]]), [data_test_batched.shape[0],1]).to(device)\n",
    "            a_p=torch.stack([torch.concat([noise[i][shift[i]:]\n",
    "                                   ,noise[i][:shift[i]]]) for i in range(data_test_batched.shape[0])]) \n",
    "        else:\n",
    "            delta_tmp = torch.reshape(delta, [1,2500])\n",
    "            noise = torch.stack([torch.concat([delta_tmp[i][shift[i]:]\n",
    "                                   ,delta_tmp[i][:shift[i]]]) for i in range(data_test_batched.shape[0])]) \n",
    "            a_p = torch.reshape(noise, [1, 1, 50, 50]).repeat([data_test_batched.shape[0],3,1,1]).to(device)\n",
    "\n",
    "        trace = torch.stack([data_test_batched[i].to(device) + a_p[i] if torch.argmax(label_test_batched, axis = 1)[i] == 1\n",
    "              else data_test_batched[i].to(device) for i in range(data_test_batched.shape[0])])\n",
    "\n",
    "        if gn:\n",
    "            trace.data = trace + torch.randn(trace.shape).to(device)\n",
    "\n",
    "        if(model_name in ['HTnet', 'SVM']):  \n",
    "            if filter_in_loop:\n",
    "                trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace.cpu().detach().numpy())]\n",
    "                trace.data = torch.Tensor(trace_tmp).to(device)\n",
    "        else:\n",
    "            if filter_in_loop:\n",
    "                trace_tmp = torch.reshape(trace[0][0], [1, 2500])\n",
    "                trace_tmp = [lowpass_firwin(x, 1024 , fh, fs, window='hamming') for x in (trace_tmp.cpu().detach().numpy())]\n",
    "                trace.data= torch.reshape(torch.Tensor(trace_tmp).to(device), [batch_size, 1, 50, 50]).repeat([data_test_batched.shape[0],3,1,1]).to(device)\n",
    "\n",
    "        if(model_name == 'SVM'):\n",
    "            pred = torch.argmax(torch_model.predict_proba(trace.type(torch.DoubleTensor).to(device)), axis = 1)\n",
    "            target  = torch.argmax(label_test_batched, axis = 1) \n",
    "        else:\n",
    "            pred = torch.argmax(torch_model(trace.to(device)), axis = 1)\n",
    "            target  = torch.argmax(label_test_batched, axis = 1) \n",
    "\n",
    "        class_accuracy = []\n",
    "\n",
    "        for label, prediction in zip(target, pred.cpu().detach().numpy()):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "                correct_pred_model += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "            total_pred_model += 1\n",
    "\n",
    "    for classname, correct_count in correct_pred.items():\n",
    "        accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "        print(f'Accuracy for class: {classname:5d} is {accuracy:.1f} %')\n",
    "        class_accuracy.append(accuracy)\n",
    "\n",
    "    accuracy_model = 100 * float(correct_pred_model) / total_pred_model\n",
    "    print(f'Model Accuracy:          {accuracy_model:.1f}%')\n",
    "    \n",
    "    return class_accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def SyncPatchPowerBudgetCal(model_name, torch_model , data_loader, nb_epoch=10, resolution=0.1, gn=False, filter_in_loop=False, fs=100, fh=20):\n",
    "    for eps in np.arange(0, 30, resolution):\n",
    "        print(\"eps := \", eps)\n",
    "        delta , losses = SyncAdversarialPatchGen(model_name, torch_model , data_loader, nb_epoch=nb_epoch, eps=eps, resolution=resolution, gn=gn, filter_in_loop=filter_in_loop, fs=fs, fh=fh)\n",
    "        class_accuracy = SyncModelEvaluation(model_name, torch_model, data_loader, delta = delta, gn = gn, filter_in_loop= filter_in_loop, fs=fs, fh=fh)\n",
    "        \n",
    "        if(class_accuracy[1] == 0):\n",
    "            break \n",
    "            \n",
    "def UnsyncPatchPowerBudgetCal(model_name, torch_model , data_loader, nb_epoch=10, resolution=0.1, gn=False, filter_in_loop=False, fs=100, fh=20):\n",
    "    for eps in np.arange(0, 30, resolution):\n",
    "        print(\"eps := \", eps)\n",
    "        delta , losses = UnsyncAdversarialPatchGen(model_name, torch_model , data_loader, nb_epoch=nb_epoch, eps=eps, resolution=resolution, gn=gn, filter_in_loop=filter_in_loop, fs=fs, fh=fh)\n",
    "        class_accuracy = UnsyncModelEvaluation(model_name, torch_model, data_loader, delta = delta, gn = gn, filter_in_loop= filter_in_loop, fs=fs, fh=fh)\n",
    "        \n",
    "        if(class_accuracy[1] == 0):\n",
    "            break \n",
    "\n",
    "\n",
    "def Train_adversarial(model_name, adv_trained_model, x_train_batched, y_train_batched, delta, nb_epoch=1):\n",
    "    \n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(adv_trained_model.parameters(), lr=1e-11, weight_decay=0.00005)\n",
    "\n",
    "    if(model_name in ['HTnet', 'SVM']):\n",
    "        noise = torch.Tensor.repeat(torch.reshape(delta, [1, x_train_batched.shape[1]]), [x_train_batched.shape[0],1]).to(device)\n",
    "\n",
    "    else:\n",
    "        noise = torch.reshape(delta, [1,1,delta.shape[0], delta.shape[1]]).repeat([1,3,1,1])#delta#torch.Tensor.repeat(torch.reshape(delta, [1, x_train_batched.shape[1]]), [x_train_batched.shape[0],1]).to(device)\n",
    "\n",
    "    trace = torch.stack([x_train_batched[i].to(device) + noise[i] if torch.argmax(y_train_batched, axis = 1)[i] == 1\n",
    "                    else x_train_batched[i].to(device) for i in range(x_train_batched.shape[0])])\n",
    "    \n",
    "    xa_val = Variable(trace.to(device))\n",
    "    ya_val = Variable(y_train_batched.to(device))\n",
    "    adv_trained_model.train()\n",
    "    for epoch in range(nb_epoch):  \n",
    "        if(model_name in ['SVM']):\n",
    "            outputs_adversarial = adv_trained_model.predict_proba(xa_val.type(torch.DoubleTensor))\n",
    "        else:\n",
    "            outputs_adversarial = adv_trained_model(xa_val)\n",
    "        loss = criterion(outputs_adversarial, ya_val)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return adv_trained_model\n",
    "\n",
    "\n",
    "def AdversarialTraining(model_name, torch_model,  data_loader, batch_size = 20, nb_epoch_noise = 10, eps = 1.2, resolution= 0.1, nb_epochs_at = 2):  \n",
    "\n",
    "    adv_trained_model = copy.deepcopy(torch_model)\n",
    "\n",
    "    for epoc in tqdm(range(nb_epochs_at)):\n",
    "        at_train_deltas = []\n",
    "        for j, (x_train_batched, y_train_batched) in (enumerate(data_loader)):\n",
    "            my_dataset_tmp  = TensorDataset(x_train_batched, y_train_batched)\n",
    "            data_loader_tmp = DataLoader(my_dataset_tmp, batch_size=batch_size)\n",
    "\n",
    "            at_train_delta, losses = SyncAdversarialPatchGen(model_name, adv_trained_model, data_loader_tmp, nb_epoch_noise, eps, resolution, gn=False, filter_in_loop=False, fs=100, fh=20, at_mode = True)\n",
    "            adv_trained_model = Train_adversarial(model_name, adv_trained_model, x_train_batched, y_train_batched, at_train_delta, nb_epoch = 1)\n",
    "\n",
    "    \n",
    "    return adv_trained_model\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "def main():\n",
    "    \n",
    "    argParser = argparse.ArgumentParser()\n",
    "    argParser.add_argument(\"-se\", \"--sync_epsilon\", type= float, default= 1.2, help= \"Adversarial noise power budget (mW)\")\n",
    "    argParser.add_argument(\"-ue\", \"--unsync_epsilon\", type= float, default= 12, help= \"Adversarial noise power budget (mW)\")\n",
    "    argParser.add_argument(\"-r\", \"--resolution\", type=float, default= 0.1, help= \"Resolution of generated patch\")\n",
    "    argParser.add_argument(\"-b\", \"--benchmark\", type=str, default='AES-T700', help=\"Benchmark name\")\n",
    "    argParser.add_argument(\"-m\", \"--model_name\", type=str, default='HTnet', help=\"Model name\")\n",
    "    argParser.add_argument(\"--sync\", action='store_true', help=\"Generate a synchronized patch\")   \n",
    "    argParser.add_argument(\"--unsync\", action='store_true', help=\"Generate a unsynchronized patch\") \n",
    "    argParser.add_argument(\"--at\", action='store_true', help=\"Generate adversarial trained model\") \n",
    "    argParser.add_argument(\"-o\", \"--output_dir\", type=str, default='./results/', help=\"Output directory\")\n",
    "    argParser.add_argument(\"--gn\", action='store_false',  default=False, help=\"Put a random guasian noise in the power trace\") \n",
    "    argParser.add_argument(\"--filter\", action='store_false', default=False, help=\"Activate filter\") \n",
    "    argParser.add_argument(\"-fs\", \"--sample_freq\", type= float, default= 100, help= \"Filter sample rate\")\n",
    "    argParser.add_argument(\"-fh\", \"--high_freq\", type= float, default= 20, help= \"Filter high frequency\")\n",
    "    argParser.add_argument(\"-f\", \"--fff\", help=\"A dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "\n",
    "    args = argParser.parse_args()\n",
    "    \n",
    "    \n",
    "    benchmark = str(args.benchmark)\n",
    "    model_name = str(args.model_name) #{'HTnet', 'ResNet-18', 'VGG-11', 'SVM'}\n",
    "    output_directory = str(args.output_dir)   \n",
    "    sync_eps = float(args.sync_epsilon)\n",
    "    unsync_eps = float(args.unsync_epsilon)\n",
    "    resolution = float(args.resolution)\n",
    "    sync = bool(args.sync)\n",
    "    unsync = bool(args.unsync)\n",
    "    at = bool(args.at)\n",
    "    \n",
    "    filter_in_loop = bool(args.filter)\n",
    "    gn = bool(args.gn)\n",
    "    fs = float(args.sample_freq)\n",
    "    fh = float(args.high_freq)\n",
    "    \n",
    "    print('Benchmark:=', benchmark)\n",
    "    print('Model:=', model_name)\n",
    "    print('Output_directory:=', output_directory)\n",
    "\n",
    "    if(device.type == 'cuda'):\n",
    "        print(\"device := GPU:\", torch.cuda.get_device_name(device))\n",
    "    else:\n",
    "        print(\"device := CPU\")\n",
    "        \n",
    "    if(filter_in_loop):\n",
    "        print(\"Sample rate of the filter:= \", fs, \"MHz\")\n",
    "        print(\"High frequency of the filter:= \", fh, \"MHz\")\n",
    "    \n",
    "    nb_epochs = 30\n",
    "    batch_size = 20\n",
    "    number_of_samples = 20000\n",
    "    workers = 16\n",
    "\n",
    "    if(model_name in ['HTnet', 'SVM']):\n",
    "        batch_size = 20\n",
    "    else:\n",
    "        batch_size = 1\n",
    "    \n",
    "    if not os.path.isdir('./dataset/' + benchmark + '_power_Temp25C') or not os.path.isdir('./trained_models/regular_models/pytorch/' + model_name):\n",
    "        print('**************************************************')\n",
    "        print('**********Downloading Model and Dataset***********')\n",
    "    \n",
    "    Preprocessing(benchmark = benchmark, model_name = model_name)\n",
    "        \n",
    "    print('**************************************************')\n",
    "    print('******************Data is loading******************')\n",
    "    create_directory(output_directory)\n",
    "    \n",
    "    data_train, label_train, data_test, label_test, input_shape, nb_classes = KerasDataPrep(benchmark = 'AES-T700', number_of_samples = number_of_samples, batch_size = batch_size)\n",
    "    # Training(input_data = data_train, label = label_train,  benchmark = 'AES-T700', output_directory = output_directory, input_shape = input_shape, nb_classes = nb_classes,  batch_size = batch_size, nb_epochs = nb_epochs, workers = workers)\n",
    "\n",
    "    torch_model = TorchLoadModel(model_name, benchmark)  \n",
    "    data_loader = TorchDataPrep(model_name, data_test, label_test)\n",
    "    delta = torch.zeros(data_test.shape[1]).to(device)\n",
    "    print('******************Data is loaded******************')\n",
    "    print('**************************************************')\n",
    "\n",
    "    print('**************************************************')\n",
    "    print('*****************Model Evaluation*****************')\n",
    "\n",
    "    ModelEvaluation(model_name, torch_model, data_loader)\n",
    "    \n",
    "    if(sync):\n",
    "        print('**************************************************')\n",
    "        print('***************Patch is generating****************')\n",
    "\n",
    "        print('Sync_epsilon:=', sync_eps)\n",
    "        print('Resolution:=', resolution)\n",
    "\n",
    "        # SyncPatchPowerBudgetCal(torch_model , data_loader, nb_epoch=10, resolution=resolution, gn=False, filter_in_loop=False, fs=100, fh=20)\n",
    "        delta , losses = SyncAdversarialPatchGen(model_name, torch_model , data_loader, nb_epoch=nb_epochs, eps=sync_eps, resolution=resolution, gn=gn, filter_in_loop=filter_in_loop, fs=fs, fh=fh)\n",
    "        SyncModelEvaluation(model_name, torch_model, data_loader, delta = delta, gn = gn, filter_in_loop= filter_in_loop, fs=fs, fh=fh)\n",
    "        create_directory(output_directory + \"patch/\" + model_name + '/' + \"sync/\")\n",
    "        np.savetxt(output_directory + \"patch/\" + model_name + '/' + \"sync/\"  + benchmark + \".txt\", delta.cpu().detach().numpy(), fmt='%.2f', delimiter='\\0')\n",
    "        \n",
    "        print('***************Patch is generated*****************')\n",
    "        print('**************************************************')\n",
    "\n",
    "    if(unsync):\n",
    "\n",
    "        print('**************************************************')\n",
    "        print('**********Unsynch patch is generating*************')\n",
    "        \n",
    "        print('unsync_epsilon:=', unsync_eps)\n",
    "        print('resolution:=', resolution)\n",
    "\n",
    "        # UnsyncPatchPowerBudgetCal(torch_model , data_loader, nb_epoch=10, resolution=resolution, gn=False, filter_in_loop=False, fs=100, fh=20)\n",
    "        delta , losses = UnsyncAdversarialPatchGen(model_name, torch_model , data_loader, nb_epoch=nb_epochs, eps=unsync_eps, resolution=resolution, gn=gn, filter_in_loop=filter_in_loop, fs=fs, fh=fh)\n",
    "        UnsyncModelEvaluation(model_name, torch_model, data_loader, delta = delta, gn = gn, filter_in_loop= filter_in_loop, fs=fs, fh=fh)\n",
    "\n",
    "        create_directory(output_directory + \"patch/\" + model_name + '/' + \"unsync/\")\n",
    "        np.savetxt(output_directory + \"patch/\" + model_name + '/' + \"unsync/\"  + benchmark + \".txt\", delta.cpu().detach().numpy(), fmt='%.2f', delimiter='\\0')\n",
    "\n",
    "        print('**********Unsynch patch is generated*************') \n",
    "        print('**************************************************')\n",
    "        \n",
    "    if(at):\n",
    "            print('**************************************************')\n",
    "            print('********Adversarial training is started***********')\n",
    "\n",
    "            at_torch_model = AdversarialTraining(model_name, torch_model, data_loader, batch_size = batch_size, nb_epoch_noise = 2, eps = sync_eps, resolution=resolution, nb_epochs_at = 3)    \n",
    "            at_torch_model.to(device)\n",
    "            SyncModelEvaluation(model_name, at_torch_model, data_loader, delta = delta, gn = False, filter_in_loop= False, fs=100, fh=20)\n",
    "\n",
    "            create_directory(output_directory + \"models/\" + model_name + '/')\n",
    "            torch.save(at_torch_model, output_directory + \"models/\" +  model_name + '/' + '/at_' + benchmark + '.pt')\n",
    "\n",
    "            print('**************Model is generated******************')\n",
    "            print('**************************************************')\n",
    "\n",
    "    # SyncPatchPowerBudgetCal(model_name, torch_model, data_loader, nb_epoch=2, resolution=0.1)\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a4092",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
